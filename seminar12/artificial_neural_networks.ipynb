{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2020_seminars/blob/master/seminar12/artificial_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 12: Artificial neural networks, MLP, PyTorch\n",
    "Machine Learning by professor Evgeny Burnaev\n",
    "\n",
    "In this seminar we will get familiar with artificial neural networks and one of the commonest frameworks to work with them --- PyTorch.\n",
    "Our humble ultimate goal will be to program and train an MLP to classify MNIST digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/adasegroup/ML2020_seminars/raw/master/seminar12/img/pic1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artificial neural network --- ANN or just NN, is simply a composition of functions with free 'trainable' parameters, or 'weights'.\n",
    "In this seminar, we will be talking about NNs that are feedforward, i.e don't have cyclic compositions of the functions,\n",
    "and we will be talking about them in the context of approximation tasks.\n",
    "\n",
    "More formally, for any functions $f_i$\n",
    "$$ f_1(x|w_1),\\quad f_2(x, f_1(x)|w_2),\\quad f_n(x, f_1(x), \\ldots, f_{n-1}(\\ldots)|w_n),$$\n",
    "$F(x) = f_n(\\ldots)$ is a feedforward neural network with weights $w_1,\\ldots,w_n$,\n",
    "and we will want it to approximate something, e.g another function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let's say that we want to find a linear fit to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_n = 100\n",
    "x = np.random.rand(points_n) * 10 - 5\n",
    "y = x * 3 + 4\n",
    "y += np.random.randn(points_n) / 2\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call function $F(x) = ax + b$ a neural network with weights $a$ and $b$, and train this neural network, which means find such values of $a$ and $b$ that $F(x)$ fits the data above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNs are trained with backpropagation algorithm, which is basically\n",
    "1. Calculate the value of the loss function, that represents how far from desired the output of the network is\n",
    "2. Calculate the derivative of the loss w.r.t to each trainable parameter of the network\n",
    "3. Somehow update the values of the parameters using these derivatives\n",
    "4. Repeat steps 1-3 for so many iterations, or until loss is low enough, or something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "Train $F(x) = ax + b$ to fit the data above. Use plain Python or NumPy, not PyTorch.\n",
    "\n",
    "More formally, we have $x_i,\\,y_i,\\,i=1,\\ldots,n_\\mathrm{pts}$\n",
    "1. Use mean squared error as the loss function $L = \\frac{1}{n_\\mathrm{pts}}\\sum_i (F(x_i) - y_i)^2$\n",
    "2. Calculate the derivatives of the loss w.r.t to $a$ and $b$\n",
    "3. Update the weights of the network using gradient descent $a \\leftarrow a - \\lambda\\partial_a L$, $b \\leftarrow b - \\lambda\\partial_b L$, where $\\lambda$ is numerical paramer called \"learning rate\"\n",
    "4. Repeat steps 1-3 until you are satisfied with the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data again\n",
    "np.random.seed(0)\n",
    "points_n = 100\n",
    "x = np.random.rand(points_n) * 10 - 5\n",
    "x.sort()  # sort points, merely for correct visualization\n",
    "y = x * 3 + 4\n",
    "y += np.random.randn(points_n) / 2\n",
    "\n",
    "# define the network and initial values of the parameters\n",
    "a = 1\n",
    "b = 2\n",
    "\n",
    "def f(x):\n",
    "    return x * a + b\n",
    "\n",
    "# monitoring\n",
    "fig = plt.figure()\n",
    "\n",
    "def visualize(fig, x, y, nn_outputs):\n",
    "    fig.clear()\n",
    "    plt.title(f'Iteration {iter_i}')\n",
    "    plt.scatter(x, y, c='tab:blue')\n",
    "    plt.plot(x, nn_outputs, c='tab:orange')\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "\n",
    "# train\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for iter_i in range(1):\n",
    "    nn_outputs = f(x)\n",
    "    \n",
    "    # calculate the derivatives\n",
    "    # Your code here\n",
    "    \n",
    "    # update the parameters\n",
    "    # Your code here\n",
    "\n",
    "    visualize(fig, x, y, nn_outputs)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.\n",
    "Now try to fit this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "points_n = 100\n",
    "x = np.random.rand(points_n) * np.pi*3 - np.pi*1.5\n",
    "x.sort()\n",
    "y = np.sin(x)\n",
    "y += np.random.randn(points_n) / 10\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a bit more complex function $F(x) = f_3(f_2(f_1(x)))$\n",
    "\n",
    "$$f_1(x) = \\mathbf{a}_1\\,x + \\mathbf{b}_1,\\quad \\mathbf{a}_1,\\,\\mathbf{b}_1\\in\\mathbb{R}^{80}, \\\\\n",
    "f_2(\\mathbf{x})_i = \\max(0, x_i),\\quad i = 1, \\ldots, 80, \\\\\n",
    "f_3(\\mathbf{x}) = \\mathbf{a}_2\\cdot\\mathbf{x} + b_2,\\quad \\mathbf{a}_2\\in\\mathbb{R}^{80},\\,b_2\\in\\mathbb{R}.\n",
    "$$\n",
    "\n",
    "The training algorithm is the same, the only basic difference is calculation of derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network and initial values of the parameters\n",
    "a1 = np.random.rand(80)\n",
    "b1 = np.random.rand(80)\n",
    "a2 = np.random.rand(80)\n",
    "b2 = 1\n",
    "\n",
    "def f(x):\n",
    "    f1_out = x[..., None] * a1 + b1  # [points_n, 80]\n",
    "    f2_out = np.maximum(f1_out, 0)\n",
    "    f3_out = np.matmul(f2_out, a2) + b2\n",
    "    return f3_out\n",
    "\n",
    "# train\n",
    "learning_rate = 1e-3\n",
    "fig = plt.figure()\n",
    "\n",
    "for iter_i in range(1):\n",
    "    nn_outputs = f(x)\n",
    "    \n",
    "    # calculate the derivatives\n",
    "    # Your code here\n",
    "    \n",
    "    # update the parameters\n",
    "    # Your code here\n",
    "\n",
    "    if iter_i % 100 == 0:\n",
    "        visualize(fig, x, y, nn_outputs)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that easy, right?\n",
    "\n",
    "The key here is to use chain rule for calculation of derivatives, or \"gradients\" since now the parameters are multidimensional\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{a}_1} = \\sum_i \\frac{\\partial L}{\\partial F(x_i)} \\frac{\\partial F(x_i)}{\\partial \\mathbf{a}_1},\\quad\n",
    "\\frac{\\partial F(x_i)}{\\partial \\mathbf{a}_1} = \\frac{\\partial F(x_i)}{\\partial f_3(x_i)}\\frac{\\partial f_3(x_i)}{\\partial f_2(x_i)}\\frac{\\partial f_2(x_i)}{\\partial f_1(x_i)}\\frac{\\partial f_1(x_i)}{\\partial \\mathbf{a}_1},\\quad \\text{etc},$$\n",
    "where\n",
    "$$L = \\frac{1}{n_\\mathrm{pts}}\\sum_i (F(x_i) - y_i)^2,\\\\\n",
    "F(x) = f_3(f_2(f_1(x))).$$\n",
    "\n",
    "The neural networks used for real-world tasks are composed of many more functions, and manually programming this each time would be impractical.\n",
    "Deep learning framewors like PyTorch provide a lot of preprogrammed functionality like this.\n",
    "\n",
    "Let's quickly look at the overview of the solution of Exercise 2 with PyTorch and then discuss the details.\n",
    "\n",
    "<font color='gray'>note for the seminarist: copy the code above and rewrite in pytorch inplace</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network and initial values of the parameters\n",
    "a1 = torch.rand(80, requires_grad=True)\n",
    "b1 = torch.rand(80, requires_grad=True)\n",
    "a2 = torch.rand(80, requires_grad=True)\n",
    "b2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "def f(x):\n",
    "    f1_out = x[..., None] * a1 + b1  # [points_n, 80]\n",
    "    f2_out = torch.max(f1_out, torch.tensor([0.]))\n",
    "    f3_out = torch.matmul(f2_out, a2) + b2\n",
    "    return f3_out\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD([a1, a2, b1, b2], lr=1e-3)\n",
    "\n",
    "# loss\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# convert data\n",
    "x = torch.as_tensor(x).type(torch.float32)\n",
    "y = torch.as_tensor(y).type(torch.float32)\n",
    "\n",
    "# train\n",
    "fig = plt.figure()\n",
    "for iter_i in range(1):\n",
    "    nn_outputs = f(x)\n",
    "    \n",
    "    # calculate the derivatives\n",
    "    loss = loss_function(nn_outputs, y)\n",
    "    \n",
    "    # update the parameters\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter_i % 100 == 0:\n",
    "        visualize(fig, x, y, nn_outputs.detach())\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of the seminar\n",
    "1. Intro to the seminar\n",
    "2. Intro to PyTorch\n",
    "3. Automatic differentiation in PyTorch\n",
    "4. PyTorch Modules\n",
    "5. Batching\n",
    "6. Putting everything together\n",
    "7. Classifying MNIST with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Intro to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is mostly like NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in numpy\n",
    "a = np.random.rand(100)\n",
    "b = np.empty(100)\n",
    "\n",
    "np.maximum(a, np.array([0.]))\n",
    "np.matmul(a, b)\n",
    "\n",
    "np.asarray([1,2,3]).astype(np.float32)\n",
    "\n",
    "# in pytorch\n",
    "a = torch.rand(100)\n",
    "b = torch.empty(100)\n",
    "\n",
    "torch.max(a, torch.tensor([0.]))\n",
    "torch.matmul(a, b)\n",
    "\n",
    "torch.as_tensor([1,2,3]).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etc.\n",
    "It is useful to look through the complete list of what there is \\[[1](https://pytorch.org/docs/stable/torch.html#math-operations), [2](https://pytorch.org/docs/stable/nn.functional.html)\\].\n",
    "For this course generally this is not necessary, but if you're going to use PyTorch for your research or the course project, there are plenty of convenient functions useful to be aware of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic differentiation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can automatically calculate derivatives of predefined functions and their compositions.\n",
    "To indicate that we want to calculate derivative of something w.r.t some tensor, we need to set the `requires_grad` flag of this tensor to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.])\n",
    "x.requires_grad_()\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $\\partial_x y(x)$ we need to call `y.backward()` (notice, not `y.backward(x)`). The value of the derivative is then stored in `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "torch.allclose(x.grad, torch.cos(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for composition of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.])\n",
    "x.requires_grad_()\n",
    "y = x\n",
    "y = torch.log(torch.exp(torch.log(torch.exp(x))))\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for multivariate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10)\n",
    "x.requires_grad_()\n",
    "coefs = torch.arange(len(x), dtype=x.dtype)\n",
    "y = torch.sum(x * coefs)\n",
    "y.backward()\n",
    "\n",
    "torch.allclose(x.grad, coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tensor already has `grad` attribute, e.g from the previous call to `backward`, subsequent calls will add to the value of this attribute.\n",
    "So, if you, e.g, `backward()` in a loop, you need to explicitely set `grad` to zero each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accumulated grad')\n",
    "x = torch.tensor([1.])\n",
    "x.requires_grad_()\n",
    "for i in 1,1:\n",
    "    y = x * 2\n",
    "    y.backward()\n",
    "    print(x.grad)\n",
    "    \n",
    "print('Not accumulated grad')\n",
    "x = torch.tensor([1.])\n",
    "x.requires_grad_()\n",
    "for i in 1,1:\n",
    "    y = x * 2\n",
    "    y.backward()\n",
    "    print(x.grad)\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors that don't require grad are transparently converted to numpy arrays.\n",
    "To convert a tensor that requires grad, we need to call its `detach` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones([10], requires_grad=True)\n",
    "# plt.plot(x)  # won't work\n",
    "plt.plot(x.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how we used it above, in our train loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    a1 = torch.rand(80, requires_grad=True)  # indicate that\n",
    "    b1 = torch.rand(80, requires_grad=True)  # we need\n",
    "    a2 = torch.rand(80, requires_grad=True)  # gradients\n",
    "    b2 = torch.rand(1, requires_grad=True)   # w.r.t to this\n",
    "\n",
    "    def f(x):\n",
    "        f1_out = x[..., None] * a1 + b1                 # if we need grad\n",
    "        f2_out = torch.max(f1_out, torch.tensor([0.]))  # w.r.t a1 then\n",
    "        f3_out = torch.matmul(f2_out, a2) + b2          # w.r.t f1_out as well\n",
    "        return f3_out\n",
    "\n",
    "    optimizer = torch.optim.SGD([a1, a2, b1, b2], lr=1e-3)\n",
    "\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "\n",
    "    x = torch.as_tensor(x).type(torch.float32)\n",
    "    y = torch.as_tensor(y).type(torch.float32)\n",
    "\n",
    "    for iter_i in range(1):\n",
    "        nn_outputs = f(x)\n",
    "\n",
    "        loss = loss_function(nn_outputs, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()        # differentiate this\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter_i % 100 == 0:\n",
    "            visualize(fig, x, y, nn_outputs.detach())  # detach for conversion to numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers a [variety of modules](https://pytorch.org/docs/stable/nn.html) representing different functions with already built-in trainable parameters.\n",
    "We can replace our manually programmed $f_1$, $f_2$ and $f_3$ with predefined modules, that are called [Linear](https://pytorch.org/docs/stable/nn.html#linear) and [ReLU](https://pytorch.org/docs/stable/nn.html#relu), i.e replace this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = torch.rand(80, requires_grad=True)\n",
    "b1 = torch.rand(80, requires_grad=True)\n",
    "a2 = torch.rand(80, requires_grad=True)\n",
    "b2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "def f(x):\n",
    "    f1_out = x[..., None] * a1 + b1\n",
    "    f2_out = torch.max(f1_out, torch.tensor([0.]))\n",
    "    f3_out = torch.matmul(f2_out, a2) + b2\n",
    "    return f3_out\n",
    "\n",
    "optimizer = torch.optim.SGD([a1, a2, b1, b2], lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return f3(f2(f1(x)))\n",
    "\n",
    "f1 = torch.nn.Linear(1, 80, bias=True)  # acts from R^1 to R^80\n",
    "f2 = torch.nn.ReLU()\n",
    "f3 = torch.nn.Linear(80, 1, bias=True)  # acts from R^80 to R^1\n",
    "\n",
    "optimizer = torch.optim.SGD([*f1.parameters(), *f3.parameters()], lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenient way of stacking the modules is with [Sequential](https://pytorch.org/docs/stable/nn.html#sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 80, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(80, 1, bias=True)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(f.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inteface of PyTorch modules allows for some batch dimensions in the input data, along which the module acts independently and in parallel.\n",
    "For instance, `Linear` and `ReLU` allow for any number of batch dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = torch.nn.Linear(1, 80, bias=True)\n",
    "\n",
    "x = torch.empty(size=[1])\n",
    "print('Shape of x:', x.shape)\n",
    "print('Shape of output:', f1(x).shape)\n",
    "\n",
    "x = torch.empty(size=[2, 3, 4, 1])\n",
    "batch_output = f1(x)\n",
    "print('Shape of x:', x.shape)\n",
    "print('Shape of output:', batch_output.shape)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        for k in range(4):\n",
    "            assert torch.allclose(\n",
    "                f1(x[i, j, k]),\n",
    "                batch_output[i, j, k]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example with sine samples, the size `points_n` corresponds to the batch dimension, since we treat each point independenlty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "torch.manual_seed(0)\n",
    "points_n = 100\n",
    "x = torch.rand(points_n, dtype=torch.float32) * np.pi*3 - np.pi*1.5\n",
    "x = x.sort()[0]\n",
    "y = torch.sin(x)\n",
    "y += torch.randn(points_n) / 10\n",
    "\n",
    "x = x[..., None]  # add data dimension\n",
    "y = y[..., None]\n",
    "print(x.shape)\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further, let's put together our training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_size = 80\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, hidden_dim_size, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_dim_size, 1, bias=True)\n",
    ")\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "fig = plt.figure()\n",
    "\n",
    "for iter_i in range(1):\n",
    "    nn_outputs = net(x)\n",
    "    loss = loss_function(nn_outputs, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter_i % 100 == 0:\n",
    "        visualize(fig, x, y, nn_outputs.detach())\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classifying MNIST with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset of images with handwritten digits.\n",
    "Each image is labeled with the corresponding number.\n",
    "We'll play around with the version which has ~2k 8x8 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = load_digits(return_X_y=True)\n",
    "\n",
    "grid = np.pad(images[:20].reshape(20, 8, 8), [[0, 0], [3, 3], [3, 3]]).reshape(2, 10, 14, 14).swapaxes(1, 2).reshape(28, -1)\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(grid, cmap='gray_r')\n",
    "targets[:20].reshape(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data for further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, images_test, targets_train, targets_test = train_test_split(images, targets, random_state=0)\n",
    "images_train = torch.from_numpy(images_train.astype(np.float32))\n",
    "images_test = torch.from_numpy(images_test.astype(np.float32))\n",
    "targets_train = torch.from_numpy(targets_train)\n",
    "targets_test = torch.from_numpy(targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'll do, we'll unroll each image into an array of 64 values and will train our $F(x)$ from above to predict an array of 10 values, each corresponding to probability of the image depicting the respective digit.\n",
    "\n",
    "More formally, $F(\\mathbf{x}) = f_3(f_2(f_1(\\mathbf{x}))),\\quad \\mathbf{x}\\in\\mathbb{R}^{64}$,\n",
    "$$f_1(\\mathbf{x}) = A_1\\cdot \\mathbf{x} + \\mathbf{b}_1,\\quad A_1\\in\\mathbb{R}^{80\\times 64},\\quad \\mathbf{b}_1\\in\\mathbb{R}^{80}, \\\\\n",
    "f_2(\\mathbf{x})_i = \\max(0, x_i),\\quad i = 1, \\ldots, 80, \\\\\n",
    "f_3(\\mathbf{x}) = A_2\\cdot\\mathbf{x} + \\mathbf{b}_2,\\quad A_2\\in\\mathbb{R}^{10\\times 80},\\,\\mathbf{b}_2\\in\\mathbb{R}^{10}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_size = 80\n",
    "input_size = 8 * 8\n",
    "number_of_classes = 10\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_size, hidden_dim_size, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_dim_size, number_of_classes, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to get probablities $p_i$ of different digits out of the network,\n",
    "we will pass the output of the network to softmax function\n",
    "$$\\mathrm{SoftMax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "to enforce the properties of probabilities,\n",
    "i.e $0 \\le p_i \\le 1$, $\\sum_i p_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_outputs = net(images_train[0])\n",
    "print(torch.all((0 <= net_outputs) & (net_outputs <= 1)))\n",
    "print(torch.allclose(net_outputs.sum(), torch.ones(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "probabilities = softmax(net_outputs)\n",
    "\n",
    "print(torch.all((0 <= probabilities) & (probabilities <= 1)))\n",
    "print(torch.allclose(probabilities.sum(), torch.ones(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then just take the index of maximal predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.max(dim=-1)[1], targets_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss) loss function, suitable for multi-class classification.\n",
    "It does conversion to probabilities by itself, so we just pass it the outputs of the network and the label of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_function(net_outputs[None], targets_train[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "Let's put everything together and try to train our network (it is MLP btw)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "# Your code here\n",
    "\n",
    "# use the same optimizer as before\n",
    "# Your code here\n",
    "\n",
    "# use the cross entropy loss function\n",
    "# Your code here\n",
    "\n",
    "# for monitoring we'll use confusion matrices on the train and test sets\n",
    "fig, [ax_train, ax_test] = plt.subplots(1, 2, figsize=[10, 5])\n",
    "\n",
    "def visualize():\n",
    "    ax_train.clear()\n",
    "    ax_train.set_title(f'Iteration {iter_i}, train')\n",
    "    ax_train.matshow(confusion_matrix(targets_train, get_predicted_classes(images_train)))\n",
    "    ax_test.set_title(f'test')\n",
    "    ax_test.matshow(confusion_matrix(targets_test, get_predicted_classes(images_test)))\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    \n",
    "def get_predicted_classes(images):\n",
    "    # Your code here\n",
    "\n",
    "for iter_i in range(1):\n",
    "    # Your code here\n",
    "    if iter_i % 10 == 0:\n",
    "        visualize()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
